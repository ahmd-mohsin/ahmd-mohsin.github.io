<!DOCTYPE HTML>
<html>
<head>
    <title>Ahsan Bilal</title>
    <meta name="author" content="Ahsan Bilal"/>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
    <link rel="stylesheet" href="assets/css/main.css"/>
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css"/>
    </noscript>

    <link rel="apple-touch-icon" sizes="57x57"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-57x57.png"/>
    <link rel="apple-touch-icon" sizes="60x60"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-60x60.png"/>
    <link rel="apple-touch-icon" sizes="72x72"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-72x72.png"/>
    <link rel="apple-touch-icon" sizes="76x76"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-76x76.png"/>
    <link rel="apple-touch-icon" sizes="114x114"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-114x114.png"/>
    <link rel="apple-touch-icon" sizes="120x120"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-120x120.png"/>
    <link rel="apple-touch-icon" sizes="144x144"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-144x144.png"/>
    <link rel="apple-touch-icon" sizes="152x152"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-152x152.png"/>
    <link rel="apple-touch-icon" sizes="180x180"
          href="https://www-media.stanford.edu/assets/favicon/apple-touch-icon-180x180.png"/>

    <link rel="icon" type="image/png" href="https://www-media.stanford.edu/assets/favicon/favicon-196x196.png"
          sizes="196x196"/>
    <link rel="icon" type="image/png" href="https://www-media.stanford.edu/assets/favicon/favicon-192x192.png"
          sizes="192x192"/>
    <link rel="icon" type="image/png" href="https://www-media.stanford.edu/assets/favicon/favicon-128.png" sizes="128x128"/>
    <link rel="icon" type="image/png" href="https://www-media.stanford.edu/assets/favicon/favicon-96x96.png" sizes="96x96"/>
    <link rel="icon" type="image/png" href="https://www-media.stanford.edu/assets/favicon/favicon-32x32.png" sizes="32x32"/>
    <link rel="icon" type="image/png" href="https://www-media.stanford.edu/assets/favicon/favicon-16x16.png" sizes="16x16"/>

    <link rel="mask-icon" href="https://www-media.stanford.edu/assets/favicon/safari-pinned-tab.svg" color="#ffffff">
    <meta name="msapplication-TileColor" content="#FFFFFF"/>
    <meta name="msapplication-TileImage" content="//www-media.stanford.edu/assets/favicon/mstile-144x144.png"/>
    <meta name="msapplication-square70x70logo" content="//www-media.stanford.edu/assets/favicon/mstile-70x70.png"/>
    <meta name="msapplication-square150x150logo" content="//www-media.stanford.edu/assets/favicon/mstile-150x150.png"/>
    <meta name="msapplication-square310x310logo" content="//www-media.stanford.edu/assets/favicon/mstile-310x310.png"/>

    <link rel="icon" type="image/png" href="favicon.ico">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-SGHM78BZFM"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-SGHM78BZFM');
    </script>
</head>
<body class="is-preload">

<article id="top" class="wrapper style1">
    <div class="container">
        <div class="row">
            <div class="col-8 col-7-large col-12-medium">
                <header>
                    <h1 class="strong">Ahsan Bilal</h1>
                </header>
                <article>
                    I am a second-year CS PhD student at <a href="https://stanford.edu/" target="_blank">University of &nbsp Oklahoma</a>,
                    advised by <a href="https://profiles.stanford.edu/fei-fei-li"
                                  target="_blank" style="white-space: nowrap">Prof. Fei-Fei
                    Li</a>. I am affiliated with the <a href="https://svl.stanford.edu/" target="_blank">Stanford Vision
                    and
                    Learning Lab</a>. I work on the intersection of machine learning and robotics.
                    <br>
                    Previously, I received my master's degree from <a href="https://stanford.edu/" target="_blank">Stanford&nbspUniversity</a>
                    and bachelor's degree from <a href="https://www.ed.ac.uk/" target="_blank">The University of
                    Edinburgh</a>.
                    I was fortunate to
                    collaborate with <a href="https://jimfan.me/" target="_blank">Dr. Jim Fan</a> and <a
                        href="https://www.cs.utexas.edu/~yukez/" target="_blank">Prof. Yuke Zhu</a>.
                    I have also worked as research intern at <a href="https://www.nvidia.com/en-us/research/"
                                                                target="_blank">NVIDIA</a> and <a
                        href="https://theaiinstitute.com/" target="_blank">Boston Dynamics AI Institute</a>.
                </article>
                <ul class="icons">
                    <li><a href="mailto:yunfanj@cs.stanford.edu" target="_blank"
                           class="icon fa-light fa-envelope" style="border-bottom: none; color: gray"
                           title="Email"></a></li>
                    <li><a href="https://scholar.google.com/citations?hl=en&user=s8Epvl4AAAAJ" target="_blank"
                           class="icon ai ai-google-scholar" style="border-bottom: none; color: gray; font-size: 110%"
                           title="Google Scholar"></a></li>
                    <li><a href="https://www.semanticscholar.org/author/Yunfan-Jiang/2171112793" target="_blank"
                           class="icon ai ai-semantic-scholar" style="border-bottom: none; color: gray; font-size: 110%"
                           title="Semantic Scholar"></a></li>
                    <li><a href="https://dblp.org/pid/311/5581-1.html" target="_blank"
                           class="icon ai ai-dblp" style="border-bottom: none; color: gray; font-size: 110%"
                           title="DBLP"></a></li>
                    <li><a href="assets/pdfs/CV_Yunfan_Jiang.pdf" target="_blank"
                           class="icon ai ai-cv-square" style="border-bottom: none; color: gray"
                           title="R&eacute;sum&eacute;"></a></li>
                    <li><a href="https://github.com/yunfanjiang" target="_blank" class="icon fab fa-github"
                           style="border-bottom: none; color: gray" title="GitHub"></a></li>
                    <li><a href="https://twitter.com/YunfanJiang" target="_blank"
                           class="icon fab fa-x-twitter" style="border-bottom: none; color: gray"
                           title="Twitter"></a></li>
                </ul>
            </div>
            <div class="col-4 col-5-large col-12-medium">
                <span class="image fit"><img src="assets/images/ahsan_bilal.png"
                                             alt="" style="border:1px solid #021a40; scale: 80%"/></span>
            </div>
        </div>
    </div>
</article>

<article class="wrapper style3">
    <div class="container">
        <header>
            <h2>News</h2>
        </header>
        <ul>
            <li><span class="date">[April 2025]</span> Organizing <a href="https://rss-moma-2025.github.io/"
                                                                     target="_blank">RSS 2025 Mobile Manipulation
                workshop</a>.
            </li>
            <li><span class="date">[March 2025]</span> Introducing <a
                    href="https://behavior-robot-suite.github.io/" target="_blank">BEHAVIOR Robot Suite</a> (BRS), a
                comprehensive framework for learning whole-body manipulation in diverse household tasks.
            </li>
            <li><span class="date">[October 2024]</span> Introducing <a
                    href="https://digital-cousins.github.io/" target="_blank">Digital Cousins</a>, which enable an
                automated real-to-sim-to-real
                pipeline for robust and scalable robot learning.
            </li>
            <li><span class="date">[September 2024]</span> <a href="https://transic-robot.github.io/"
                                                              target="_blank">T<span
                    style="font-variant-caps:all-small-caps">RANSIC</span></a> and <a
                    href="https://digital-cousins.github.io/" target="_blank">Digital Cousins</a> are accepted at <a
                    href="https://www.corl.org/" target="_blank">CoRL 2024</a>.
            </li>
            <li><span class="date">[May 2024]</span> Introducing <a href="https://transic-robot.github.io/"
                                                                    target="_blank">T<span
                    style="font-variant-caps:all-small-caps">RANSIC</span></a>, a holistic
                human-in-the-loop method to tackle sim-to-real transfer for contact-rich robot manipulation.
            </li>
            <li><span class="date">[March 2024]</span> <a href="https://voyager.minedojo.org/"
                                                          target="_blank">Voyager</a> is accepted at TMLR.
            </li>
            <li>
                <span class="date">[September 2023]</span> One paper called <a href="https://cec-agent.github.io/"
                                                                               target="_blank">Cross-Episodic
                Curriculum</a> is accepted at <a
                    href="https://nips.cc/Conferences/2023" target="_blank">NeurIPS 2023</a>. See you again in
                NOLA.
            </li>
            <li>
                <span class="date">[June 2023]</span> I am glad to receive the prestigious <span
                    class="research_highlight">Stanford School of Engineering Exceptional Masterâ€™s Student Award</span>
                <a href="https://ee.stanford.edu/2023-commencement-ceremony-and-awards"
                   target="_blank">[press]</a>.
            </li>
            <li>
                <span class="date">[June 2023]</span> I will be interning at <a href="https://theaiinstitute.com/"
                                                                                target="_blank">Boston Dynamics AI
                Institute</a> this summer.
            </li>
            <li>
                <span class="date">[May 2023]</span> We introduce <a href="https://voyager.minedojo.org/"
                                                                     target="_blank">Voyager</a>, the first
                LLM-powered embodied lifelong learning agent in an open-ended Minecraft world.
            </li>
            <li>
                <span class="date">[April 2023]</span> <a href="https://vimalabs.github.io/" target="_blank">VIMA</a>
                is accepted at <a
                    href="https://icml.cc/Conferences/2023" target="_blank">ICML 2023</a>. I'm also glad to
                receive the conference travel grant. See y'all in Hawaii!
            </li>
            <li>
                <span class="date">[April 2023]</span> ðŸ“¢ Life Update ðŸ“¢ Decide to stay at Stanford for my PhD study!
            </li>
            <li>
                <span class="date">[November 2022]</span> I talk about <a href="https://vimalabs.github.io/"
                                                                          target="_blank">VIMA</a> in <a
                    href="https://nips.cc/virtual/2022/workshop/49988/" target="_blank">Foundation Models for
                Decision Making</a> workshop at NeurIPS 2022.
            </li>
            <li>
                <span class="date">[November 2022]</span> <a href="https://minedojo.org/" target="_blank">MineDojo</a>
                is accepted at NeurIPS 2022 D&B
                track and wins the <span class="research_highlight">Outstanding Paper Award</span>.
            </li>
            <li>
                <span class="date">[October 2022]</span> We design a Transformer agent, <a
                    href="https://vimalabs.github.io/" target="_blank">VIMA</a>,
                that ingests multimodal prompts and solves a wide variety of robot manipulation tasks.
            </li>
            <li>
                <span class="date">[June 2022]</span> We release a new framework called <a href="https://minedojo.org/"
                                                                                           target="_blank">MineDojo</a>
                for building generalist agents that can solve open-ended tasks with internet-scale knowledge in
                Minecraft.
            </li>

        </ul>
    </div>
</article>

<article class="wrapper style2">
    <div class="container">
        <header>
            <h2>Research <span class="toggle-pub" id="toggle-pub-container">[<a href="javascript:void(0);"
                                                                                id="show-selected">representative</a> | <a
                    href="javascript:void(0);" id="show-all">all</a>]</span>
                <!--                <br>-->
                <!--                <span class="toggle-pub">Topics: <a href="javascript:void(0);"-->
                <!--                                                    onclick="clickAnyResearchTopic();scrollToSection('rl_syn')">Robot Learning from Synthetic Data</a> | <a-->
                <!--                        href="javascript:void(0);" onclick="clickAnyResearchTopic();scrollToSection('fmdm')">Foundation Models for Decision Making</a></span>-->
            </h2>
        </header>

        <div class="row" id="publications">
            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="true"
                 data-topic="rl_syn">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/brs.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="true"
                 data-topic="rl_syn">
                <article style="text-align: left">
                    <h3>
                        BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household
                        Activities
                    </h3>
                    <span
                            class="strong">Yunfan Jiang</span>, <a href="https://ai.stanford.edu/~zharu/"
                                                                   target="_blank">Ruohan Zhang</a>, <a
                        href="https://jdw.ong/" target="_blank">Josiah Wong</a>, <a
                        href="https://www.chenwangjeremy.net/"
                        target="_blank">Chen Wang</a>, <a href="https://yanjieze.com/" target="_blank">Yanjie Ze</a>, <a
                        href="https://hang-yin.github.io/" target="_blank">Hang Yin</a>, <a
                        href="https://www.cemgokmen.com/" target="_blank">Cem Gokmen</a>, <a
                        href="https://shurans.github.io/" target="_blank">Shuran Song</a>, <a
                        href="https://jiajunwu.com/"
                        target="_blank">Jiajun
                    Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li"
                               target="_blank">Li
                    Fei-Fei</a>
                    <br>
                    <span class="venue">arXiv 2025</span>
                    <br>
                    <a href="https://arxiv.org/abs/2503.05652" target="_blank" class="links">arXiv</a> | <a
                        href="https://behavior-robot-suite.github.io/assets/pdf/brs_paper.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://behavior-robot-suite.github.io/" target="_blank" class="links">website</a> | <a
                        href="https://behavior-robot-suite.github.io/docs/" target="_blank" class="links">doc</a> | <a
                        href="https://github.com/behavior-robot-suite/brs-algo" target="_blank" class="links">algorithm
                    code</a> | <a href="https://github.com/behavior-robot-suite/brs-ctrl" target="_blank" class="links">hardware
                    code</a> | <a
                        href="https://x.com/YunfanJiang/status/1899113026663682432" target="_blank"
                        class="links">tldr</a>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="false"
                 data-topic="rl_syn">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/acdc.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="false"
                 data-topic="rl_syn">
                <article style="text-align: left">
                    <h3>
                        Automated Creation of Digital Cousins for Robust Policy Learning
                    </h3>
                    <a href="https://rogerdai1217.github.io/" target="_blank">Tianyuan Dai</a><sup>*</sup>, <a
                        href="https://jdw.ong/" target="_blank">Josiah Wong</a><sup>*</sup>, <span
                        class="strong">Yunfan Jiang</span>, <a
                        href="https://www.chenwangjeremy.net/"
                        target="_blank">Chen Wang</a>, <a href="https://www.cemgokmen.com/" target="_blank">Cem
                    Gokmen</a> <a href="https://ai.stanford.edu/~zharu/"
                                  target="_blank">Ruohan Zhang</a>, <a
                        href="https://jiajunwu.com/"
                        target="_blank">Jiajun
                    Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li"
                               target="_blank">Li
                    Fei-Fei</a>
                    <br>
                    <span class="author_footnote">* Equal contribution</span>
                    <br>
                    <span class="venue">CoRL 2024</span>
                    <br>
                    <a href="https://arxiv.org/abs/2410.07408" target="_blank" class="links">arXiv</a> | <a
                        href="https://digital-cousins.github.io/assets/paper.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://digital-cousins.github.io/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/cremebrule/digital-cousins" target="_blank" class="links">code</a> | <a
                        href="https://x.com/RogerDai1217/status/1844411408374693941" target="_blank"
                        class="links">tldr</a>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="true"
                 data-topic="rl_syn">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/transic.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="true"
                 data-topic="rl_syn">
                <article style="text-align: left">
                    <h3>
                        T<span style="font-variant-caps:all-small-caps">RANSIC</span>: Sim-to-Real
                        Policy Transfer by Learning from Online Correction
                    </h3>
                    <span
                            class="strong">Yunfan Jiang</span>, <a
                        href="https://www.chenwangjeremy.net/"
                        target="_blank">Chen Wang</a>, <a href="https://ai.stanford.edu/~zharu/"
                                                          target="_blank">Ruohan Zhang</a>, <a
                        href="https://jiajunwu.com/"
                        target="_blank">Jiajun
                    Wu</a>, <a href="https://profiles.stanford.edu/fei-fei-li"
                               target="_blank">Li
                    Fei-Fei</a>
                    <br>
                    <span class="venue">CoRL 2024</span>
                    <br>
                    <a href="https://arxiv.org/abs/2405.10315" target="_blank" class="links">arXiv</a> | <a
                        href="https://transic-robot.github.io/assets/pdf/transic_paper.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://transic-robot.github.io/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/transic-robot/transic" target="_blank" class="links">code</a> | <a
                        href="https://x.com/YunfanJiang/status/1791498916548272489" target="_blank"
                        class="links">tldr</a>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="false"
                 data-topic="fmdm">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/openx.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="false"
                 data-topic="fmdm">
                <article style="text-align: left">
                    <h3>
                        Open X-Embodiment: Robotic Learning Datasets and RT-X Models
                    </h3>
                    Open X-Embodiment Collaboration
                    <br>
                    <span class="venue">ICRA 2024</span>
                    <br>
                    <a href="https://arxiv.org/abs/2310.08864" target="_blank" class="links">arXiv</a> | <a
                        href="https://ieeexplore.ieee.org/document/10611477"
                        target="_blank" class="links">paper</a> | <a
                        href="https://robotics-transformer-x.github.io/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/google-deepmind/open_x_embodiment" target="_blank"
                        class="links">data</a> | <a
                        href="https://x.com/GoogleDeepMind/status/1709207886943965648" target="_blank" class="links">tldr</a>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="true"
                 data-topic="fmdm">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/voyager.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="true"
                 data-topic="fmdm">
                <article style="text-align: left">
                    <h3>
                        Voyager: An Open-Ended Embodied Agent with Large Language Models
                    </h3>
                    <a href="https://www.guanzhi.me/" target="_blank">Guanzhi Wang</a>, <a
                        href="https://xieleo5.github.io/" target="_blank">Yuqi Xie</a>, <span
                        class="strong">Yunfan
                    Jiang</span><sup>*</sup>, <a
                        href="https://ai.stanford.edu/~amandlek/" target="_blank">Ajay Mandlekar</a><sup>*</sup>, <a
                        href="https://xiaocw11.github.io/" target="_blank">Chaowei&nbspXiao</a>, <a
                        href="https://www.cs.utexas.edu/~yukez/" target="_blank">Yuke&nbspZhu</a>, <a
                        href="https://jimfan.me/" target="_blank">Linxi&nbsp"Jim"&nbspFan</a><sup>&dagger;</sup>,
                    <a
                            href="http://tensorlab.cms.caltech.edu/users/anima/"
                            target="_blank">Anima&nbspAnandkumar</a><sup>&dagger;</sup>
                    <br>
                    <span class="author_footnote">* Equal contribution&nbsp; &nbsp;&dagger; Equal advising</span>
                    <br>
                    <span class="venue">TMLR</span>
                    <br>
                    <span class="venue_footnote">Also <span class="research_highlight">Oral Presentation</span> at NeurIPS 2023 <a
                            href="https://sites.google.com/view/aloe2023/home" target="_blank">Agent Learning in Open-Endedness Workshop</a> and <a
                            href="https://imol-workshop.github.io/home/" target="_blank">Intrinsically Motivated Open-Ended Learning Workshop</a></span>
                    <br>
                    <a href="https://arxiv.org/abs/2305.16291" target="_blank" class="links">arXiv</a> | <a
                        href="https://voyager.minedojo.org/assets/documents/voyager.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://voyager.minedojo.org/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/MineDojo/Voyager" target="_blank" class="links">code</a> | <a
                        href="https://x.com/DrJimFan/status/1662115266933972993?s=20" target="_blank"
                        class="links">tldr</a>
                    <br>
                    <span class="media_coverage">Covered by <a
                            href="https://www.forbes.com/sites/johnkoetsier/2023/05/29/gpt-4-is-pretty-good-at-minecraft/"
                            target="_blank">Forbes</a>, <a
                            href="https://www.wired.com/story/fast-forward-gpt-4-minecraft-chatgpt/" target="_blank">WIRED</a>, and <a
                            href="https://youtu.be/VKEA5cJluc0" target="_blank">Two Minute Papers</a></span>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="false"
                 data-topic="fmdm">
                <article>
                    <div class="image fit"><img src="assets/images/research/cec.png" id="cec_image" alt=""/></div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="false"
                 data-topic="fmdm">
                <article style="text-align: left">
                    <h3>
                        Cross-Episodic Curriculum for Transformer Agents
                    </h3>
                    <a href="https://lucys0.github.io/"
                       target="_blank">Lucy Xiaoyang Shi</a><sup>*</sup>, <span
                        class="strong">Yunfan Jiang</span><sup>*</sup>, <a href="https://jakegrigsby.github.io/"
                                                                           target="_blank">Jake Grigsby</a>, <a
                        href="https://jimfan.me/" target="_blank">Linxi&nbsp"Jim"&nbspFan</a><sup>&dagger;</sup>, <a
                        href="https://www.cs.utexas.edu/~yukez/" target="_blank">Yuke&nbspZhu</a><sup>&dagger;</sup>
                    <br>
                    <span class="author_footnote">* Equal contribution&nbsp; &nbsp;&dagger; Equal advising</span>
                    <br>
                    <span class="venue">NeurIPS 2023</span>
                    <br>
                    <a href="https://arxiv.org/abs/2310.08549" target="_blank" class="links">arXiv</a> | <a
                        href="https://arxiv.org/pdf/2310.08549.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://cec-agent.github.io/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/CEC-Agent/CEC" target="_blank" class="links">code</a> | <a
                        href="https://x.com/lucy_x_shi/status/1712908194237108270?s=20" target="_blank" class="links">tldr</a>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="true"
                 data-topic="rl_syn">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/vima.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="true"
                 data-topic="rl_syn">
                <article style="text-align: left">
                    <h3>
                        VIMA: General Robot Manipulation with Multimodal Prompts
                    </h3>
                    <span class="strong">Yunfan Jiang</span>, <a href="http://web.stanford.edu/~agrim/"
                                                                 target="_blank">Agrim Gupta</a><sup>*</sup>, <a
                        href="https://zcczhang.github.io/" target="_blank">Zichen "Charles" Zhang</a><sup>*</sup>, <a
                        href="https://www.guanzhi.me/" target="_blank">Guanzhi Wang</a><sup>*</sup>, <a
                        href="http://group.iiis.tsinghua.edu.cn/~milab/person-douyongqiang.html" target="_blank">Yongqiang
                    Dou</a>, <a href="https://www.linkedin.com/in/yanjun-anthony-chen/" target="_blank">Yanjun Chen</a>,
                    <a href="https://profiles.stanford.edu/fei-fei-li"
                       target="_blank">Li&nbspFei-Fei</a>, <a
                        href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Anima&nbspAnandkumar</a>,
                    <a
                            href="https://www.cs.utexas.edu/~yukez/" target="_blank">Yuke&nbspZhu</a><sup>&dagger;</sup>,
                    <a
                            href="https://jimfan.me/" target="_blank">Linxi&nbsp"Jim"&nbspFan</a><sup>&dagger;</sup>
                    <br>
                    <span class="author_footnote">* Equal contribution&nbsp; &nbsp;&dagger; Equal advising</span>
                    <br>
                    <span class="venue">ICML 2023</span>
                    <br>
                    <span class="venue_footnote">Also <span class="research_highlight">Oral Presentation</span> at <a
                            href="https://nips.cc/virtual/2022/workshop/49988/" target="_blank">NeurIPS 2022 Foundation Models for Decision Making Workshop</a></span>
                    <br>
                    <a href="https://arxiv.org/abs/2210.03094" target="_blank" class="links">arXiv</a> | <a
                        href="https://vimalabs.github.io/assets/vima_paper.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://vimalabs.github.io/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/vimalabs/VIMA" target="_blank" class="links">code</a> | <a
                        href="https://x.com/DrJimFan/status/1578433493561769984?s=20" target="_blank"
                        class="links">tldr</a>
                    <br>
                    <span class="media_coverage">Covered by <a
                            href="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52226/"
                            target="_blank">NVIDIA GTC Jensen Huang Keynote</a></span>
                </article>
            </div>

            <div class="col-4 col-4-medium col-4-small vertically_aligned pub-card" data-selected="true"
                 data-topic="fmdm">
                <article>
                    <div class="image fit">
                        <video autoplay muted playsinline loop preload="metadata">
                            <source src="assets/images/research/minedojo.mp4" type="video/mp4">
                        </video>
                    </div>
                </article>
            </div>
            <div class="col-8 col-8-medium col-8-small vertically_aligned pub-card" data-selected="true"
                 data-topic="fmdm">
                <article style="text-align: left">
                    <h3>
                        MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge
                    </h3>
                    <a href="https://jimfan.me/" target="_blank">Linxi&nbsp"Jim"&nbspFan</a>, <a
                        href="https://www.guanzhi.me/" target="_blank">Guanzhi Wang</a><sup>*</sup>, <span
                        class="strong">Yunfan
                    Jiang</span><sup>*</sup>, <a
                        href="https://ai.stanford.edu/~amandlek/" target="_blank">Ajay Mandlekar</a>,
                    <a href="https://yyuncong.github.io/" target="_blank">Yuncong Yang</a>, <a
                        href="https://www.haoyizhu.site/" target="_blank">Haoyi Zhu</a>, <a
                        href="https://twitter.com/andrewtang01" target="_blank">Andrew Tang</a>, <a
                        href="https://ai.stanford.edu/~dahuang/" target="_blank">De-An Huang</a>, <a
                        href="https://www.cs.utexas.edu/~yukez/" target="_blank">Yuke Zhu</a><sup>&dagger;</sup>, <a
                        href="http://tensorlab.cms.caltech.edu/users/anima/"
                        target="_blank">Anima Anandkumar</a><sup>&dagger;</sup>
                    <br>
                    <span class="author_footnote">* Equal contribution&nbsp; &nbsp;&dagger; Equal advising</span>
                    <br>
                    <span class="venue">NeurIPS 2022 Datasets and Benchmarks Track</span>
                    <br>
                    <span class="research_highlight">ðŸŽ‰ Outstanding Paper Award ðŸŽ‰</span>
                    <br>
                    <span class="research_highlight">Featured Paper (Oral-Equivalent)</span>
                    <br>
                    <a href="https://arxiv.org/abs/2206.08853" target="_blank" class="links">arXiv</a> | <a
                        href="https://papers.nips.cc/paper_files/paper/2022/file/74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf"
                        target="_blank" class="links">paper</a> | <a
                        href="https://minedojo.org/" target="_blank" class="links">website</a> | <a
                        href="https://github.com/MineDojo" target="_blank" class="links">code</a> | <a
                        href="https://x.com/DrJimFan/status/1595459499732926464?s=20" target="_blank"
                        class="links">tldr</a>
                    <br>
                    <span class="media_coverage">Covered by <a
                            href="https://developer.nvidia.com/blog/building-generally-capable-ai-agents-with-minedojo/"
                            target="_blank">NVIDIA Blog</a>, <a href="https://www.youtube.com/watch?v=5LL6z1Ganbw"
                                                                target="">Two Minute Papers</a>, and <a
                            href="https://arstechnica.com/information-technology/2022/11/nvidia-wins-award-for-ai-that-can-play-minecraft-on-command/"
                            target="_blank">Ars Technica</a></span>
                </article>
            </div>

        </div>
    </div>
</article>

<article class="wrapper style4">
    <div class="container medium">
        <footer>
            <ul id="copyright">
                <li>&copy; 2021 - 2024 Yunfan Jiang</li>
            </ul>
        </footer>
    </div>
</article>


<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.scrollex.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>
<script src="assets/js/toggle_pub.js"></script>

</body>
</html>
