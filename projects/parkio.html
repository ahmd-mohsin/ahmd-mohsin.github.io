<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Deep Reinforcement Learning for Resource Allocation in Wireless Networks </title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="../assets/img/favicon.png" rel="icon">
  <link href="../assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="../assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="../assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="../assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="../assets/vendor/venobox/venobox.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="../assets/css/main/style.css" rel="stylesheet">

  <!-- =======================================================
    * Template Name: Personal - v2.2.0
    * Template URL: https://bootstrapmade.com/personal-free-resume-bootstrap-template/
    * Author: BootstrapMade.com
    * License: https://bootstrapmade.com/license/
    ======================================================== -->
</head>

<body data-gr-c-s-loaded="true">

  <!-- ======= Portfolio Details ======= -->
  <main id="main">
    <div id="portfolio-details" class="portfolio-details">
      <div class="container">

        <div class="row">
          <div class="col-lg-12 portfolio-info">
            <br>
            <h2 style="color:#12d640">Deep Reinforcement Learning for Resource Allocation in Wireless Networks</h2>
            <ul>
              <li><strong>Tech Stack</strong>: Rllib, Jax, Reinforcement Learning, Wireless Communications.</li>
              <li><strong>Github URL</strong>: <a
                  href="https://github.com/muhd-umer/rl-wireless"
                  target="_blank">Project
                  Link</a>
              </li>
            </ul>
            <p>
              Dynamic resource allocation is a critical problem in wireless communication systems. It is the process of allocating
              resources such as power, bandwidth, and time slots to users in a way that maximizes the overall system performance. Traditionally, dynamic resource allocation has been done using centralized algorithms. These algorithms require a central controller to have knowledge of the system state and to
              make decisions for all users. However, centralized algorithms are not scalable to large-scale wireless networks.
              
            </p>
            <p>
              This project investigates the application of deep
              reinforcement learning (DRL) algorithms for dynamic resource
              allocation in wireless communication systems. We first created
              an environment that includes a base station, multiple antennas,
              and user equipment. Then, we utilized the RLlib library to
              pass this environment to various DRL algorithms such as DQN
              and PPO (Proximal Policy Optimization).
            </p>
            <p>
              The algorithms were
              compared based on their ability to optimize resource allocation,
              and we specifically examined the impact of different learning
              rates and scheduling policies.Our results show that the choice
              of algorithm and learning rate significantly affect the system's
              performance, and the use of DRL algorithms can provide more
              efficient resource allocation compared to traditional methods.
            </p>
            <p>
              Specifically, our experiments show that R2D2, a variant of
              DQN, outperforms both DQN and PPO in terms of achieving
              the desired reward most stably and the fastest. Our results also
              highlight the importance of choosing an appropriate learning
              rate, as the use of two separate learning rates in PPO did not
              improve its performance compared to using a single learning rate.

            </p>
            <p>
              Members: Muhammad Umer, Tariq Umer.
            </p>
            <p>
              More information available on GitHub.
            </p>
          </div>
        </div>

      </div>
    </div><!-- End Portfolio Details -->
  </main><!-- End #main -->

  <!-- Vendor JS Files -->
  <script async="" src="//www.google-analytics.com/analytics.js"></script>
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="../assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="../assets/vendor/php-email-form/validate.js"></script>
  <script src="../assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script src="../assets/vendor/counterup/counterup.min.js"></script>
  <script src="../assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="../assets/vendor/venobox/venobox.min.js"></script>

  <!-- Template Main JS File -->
  <script src="../assets/js/main.js"></script>

</body>

</html>